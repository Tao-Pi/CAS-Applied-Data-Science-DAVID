{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b694437-bf18-4692-b6bc-8ba3f47aaef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notebook 2, Module 2, Statistical Inference for Data Science, CAS Applied Data Science, 2019-08-27, G. Conti, S. Haug, University of Bern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb9674e-aa91-4ee0-bffa-66f8bfb895e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 2. Probability and descriptive statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3526ee2b-0677-4d57-b665-4063172196ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Average expected study time :** 3x45 min (depending on your background). Your are supposed to play with the examples: change them, maybe test on another dataset. From just executing them, you will not learn much.\n",
    "\n",
    "**Learning outcomes :**\n",
    "- Know difference between frequentist and Bayesian interpretation of probability\n",
    "- Random variables and probability density functions\n",
    "  - Know the normal probability density function (p.d.f.) by heart\n",
    "- Describing data with descriptive statistics\n",
    "  - Obtain moments of a p.d.f. (mean, variance, standard deviation, kurtosis, skewness, quantile, median, mode)\n",
    "  - Plot a p.d.f and its moments\n",
    "  - Generate data randomly according to a p.d.f.\n",
    "- Know the existence of some other important p.d.f.\n",
    "- Understand the meaning of gaussian/normal uncertainties\n",
    "\n",
    "#### Literature\n",
    "\n",
    "Introduction slides : \n",
    "- https://docs.google.com/presentation/d/1JOlZHsnICU5cziUoziLwcog51Jt16XnNZ9WVQJJVink/edit#slide=id.g3f94386453_0_671\n",
    "\n",
    "- An introduction: http://www.wormbook.org/chapters/www_statisticalanalysis/statisticalanalysis.pdf\n",
    "- Python: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "- R: https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/data.frame\n",
    "- Introduction to statistical data analysis for the life sciences, Claus Thorn Ekstrøm, Helle Sørensen\n",
    "- Statistics for the life sciences, Myra L. Samuels et al.\n",
    "\n",
    "For mathematicians or very interested people\n",
    "- A. Stuart, J.K. Ord, and S. Arnold, Kendall’s Advanced Theory of Statistics\n",
    "\n",
    "### Exercise 2.0  (10 sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc326185-1927-4b9b-ac14-749fb5314c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the needed python libraries by executing this python code (press ctrl enter)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "print('Congrats, you just loaded numpy, scipy.stats and mathplot.pyplot loaded !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d8de570-1de6-4a54-b943-bcce7aa17a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 Random variables and probability density functions (p.d.f.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e7be9ea-e3dd-4f35-9e33-845e3f88a4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In practice the measurement process, the data taking, is a random, or stochastic, process. The outcome varies from measurement to measurement. There are three (at least) reasons:\n",
    "\n",
    "- Measurements are normally on a sample, not the full population. Samples fluctuate.\n",
    "- Sensors have limited resolution, measurements on the same sample vary within the resolution \n",
    "- According to quantum mechanics, i.e. at smallest distances, measurements are by nature stochastic\n",
    "\n",
    "Our Iris dataset is a sample from 50 flowers in each class. So in each class there are 50 varying measurements for each of the four observables, sepal and petal length and width. This is due to the first and maybe the second reason (quantum mechanics can be neclected at scales larger than molecules). In descriptive statistics the observables are therefore called **random** variables. Let us call one x for examplification.\n",
    "\n",
    "If x can take on any value from a continuous range, we write $f(x;\\theta)dx$ as the probability that the measurement’s outcome lies between x and $x + dx$. The function $f (x; \\theta)$ is called the **probability density function (p.d.f.)**, which may depend on one or more parameters $\\theta$ (for example the Iris class).\n",
    "\n",
    "A random variable can be discrete or continuous. If discrete, then we use $f(x;\\theta)$ to denote the probability to find the value x (in python the term probability mass fundtion, pmf, is then used). In the following the term p.d.f. is often taken to cover both the continuous and discrete cases, although technically the term density should only be used in the continuous case.\n",
    "\n",
    "The p.d.f. is always normalized to **unity** (the number 1), i.e. the integral or the surface under the curve equals one. Both x and $\\theta$ may have multiple components and are then often written as vectors. If $\\theta$ is unknown, we may wish to estimate its value from a given set of measurements of x; this is a central topic of statistics (see next notebook on parameter estimation and regression).\n",
    "\n",
    "The p.d.f. should be chosen to describe the fluctuation of the random variable in a best possible way. In other words, we should always choose an approprate p.d.f to describe our data. Some very useful and much used p.d.f. follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dae6ad93-7871-4e66-8e90-14e81aa42573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The normal p.d.f.\n",
    "\n",
    "The normal (or Gaussian) probability density function is probably the most used one (informally the \"bell curve\"). It \n",
    "derives its importance in large part from the *central limit theorem*: \"In most situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a \"bell curve\") even if the original variables themselves are not normally distributed.\" https://en.wikipedia.org/wiki/Central_limit_theorem \n",
    "\n",
    "**Example:** If one flips a coin many times the probability of getting a given number of heads in a series of flips will approach a normal curve, with mean equal to half the total number of flips in each series. (In the limit of an infinite number of flips, it will equal a normal curve.)\n",
    "\n",
    "This means that in many or most cases it is sufficient to know the characteristics of the normal p.d.f. Others can be looked up if needed. Also often unspecified statements like the *error*, or better, the *uncertainty* refer to their meaning on the normal p.d.f.\n",
    "\n",
    "As a formula the normal distribution function looks like (in one dimension)\n",
    "\n",
    "$$ f(x;\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp(\\frac{-(x-\\mu)^2}{2\\sigma^2}) $$\n",
    "\n",
    "It reads, given the distribution parameters mean $\\mu$ and variance $\\sigma$, x follows this function.\n",
    "\n",
    "### Exercise 2.1 (5 min)\n",
    "\n",
    "Plot the normal distribution with mean 0 and variance 5 for 400 x values between -20 to 20. Repeat this for two other means and variances. How big is the surface under the curves ?\n",
    "\n",
    "(See also scipy.stat.norm https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.stats.norm.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c83bb91-317d-4eb4-98ef-9058305662a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Part of the solution:\n",
    "x = np.linspace(-20,20,400) # 400 bins from -20 to 20\n",
    "plt.plot(x, scipy.stats.norm.pdf(x,0,5))\n",
    "plt.plot(x, scipy.stats.norm.pdf(x,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04a849b-8849-424b-b951-6b586f32ca19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Some words on probability and bayesian versus frequentist statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a69b20-3c18-46ea-81d3-c269b24e5e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since data taking is data taking of random variables, we need to define and talk about probability. In mathematichs probability is defined in a rather abstract manner (see Annex below). For our purposes we go directly to the interpreation as either **relative frequency** or **subjective probability**. If A is a possible outcome of an experiment repeated n times, then the probability of A is the realtive frequency\n",
    "\n",
    "$$P(A) = \\lim_{n\\rightarrow \\infty} \\frac{times \\; outcome \\;is\\;A}{n}$$\n",
    "\n",
    "The subjective probability is\n",
    "\n",
    "$$ P(A) = degree\\;of\\;belief\\;that \\;A \\; is\\; true$$\n",
    "\n",
    "Both concepts are consistent with the abstract mathematical definition .\n",
    "\n",
    "#### Bayes' theorem\n",
    "\n",
    "From this definition and using the fact that $A ∩ B$ and $B ∩ A$ (intersection) are the same, one obtains\n",
    "Bayes’ theorem\n",
    "\n",
    "$$P (A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "first published by the Reverend Thomas Bayes (1702-1761). Statistics based on the relative frequency interpretation of probability is called frequentist statistics, on bayesian theorem bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdabe83c-8a58-47f5-8606-c58018762da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3 Describing the data with descriptive statistics\n",
    "\n",
    "Statistics is a branch of mathematics dealing with the collection, analysis, interpretation, presentation, and organization of data. Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation (see moments), and inferential statistics, which draw conclusions from data that are subject to random variation (e.g. observational errors, sampling variation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55b64c8c-37e4-4875-84d6-c518ad1089af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3.1 Moments of the p.d.f. (mean, variance and standard deviation)\n",
    "\n",
    "The $n^{th}$ moment of a random variable x with p.d.f. $f_(x)$ is\n",
    "$$ \\alpha_n \\equiv E[x^n] = \\int_{-\\infty}^{\\infty} x^nf(x)dx   $$\n",
    "In the discrete case this integral becomes the sum known as the arithemtic mean:\n",
    "$$ \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i$$\n",
    "\n",
    "<!--\n",
    "and the $n^{th}$ central moment of x (or moment about the mean, $α_1$) is\n",
    "\n",
    "$$ m_n \\equiv E[(x-\\alpha_1)^n] = \\int_{-\\infty}^{\\infty} (x-\\alpha_1)^nf(x)dx   $$\n",
    "-->\n",
    "\n",
    "The most commonly used moments are the **mean $\\mu$ (or expectation value) and variance $\\sigma^2$**:\n",
    "\n",
    "$$\\mu \\equiv \\alpha_1 $$\n",
    "$$\\sigma^2 \\equiv V[x] \\equiv \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)dx = ... = \\alpha_2 - \\mu^2 $$\n",
    "\n",
    "The mean is the location of the “center of mass” of the p.d.f., and the variance is a measure of the square of its width. It is often convenient to use the **standard deviation (SD)** of $x$, $\\sigma$, defined as the square root of the variance. In the discrete case the variance becomes\n",
    "$$ \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$$\n",
    "\n",
    "For the normal p.d.f the standard deviation is its width. \n",
    "\n",
    "Based on higher moments other distribution descriptors are formed. Skewness and kurtosis you may encounter. The skewness is a number indicating the deviation from a symmetric form. Kurtosis is a number indicating if the tails of the distribution is larger or smaller then the tails of the normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e69156dd-a53d-4f51-b7f9-85b573610cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3.2 Quantiles and median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5646cadc-9367-46cd-bec2-71ba8ff6113a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The **quantile $x_{\\alpha}$** is the value of the random variable x at which $\\alpha$% of the area is below x. An important special case is the **median, $x_{med} \\equiv x_{50}$**. At the median half the area lies above and half lies below. \n",
    "For the normal p.d.f. the median equals the mean. The most probable value of a distribution is called **mode**. \n",
    "\n",
    "Special quantiles are the quartiles and percentiles. The first quartile is the $x_{25}$, the second the $x_{50}$ etc. Percentiles are for example $x_{13}$ etc. \n",
    "\n",
    "<!--\n",
    "Any odd moment about the mean is a measure of the **skewness** of the p.d.f. The simplest of these is the dimensionless coefficient of skewness $\\gamma_1 = m_33/\\sigma^3$.\n",
    "\n",
    "The fourth central moment $m_4$ provides a convenient measure of the tails of a distribution. For the Gaussian distribution, one has $m_4 = 3\\sigma^4$. The **kurtosis** is defined as $\\gamma_2 = m_4/\\sigma^4 − 3$, i.e., it is zero for a Gaussian, positive for a leptokurtic distribution with longer tails, and negative for a platykurtic distribution with tails that die off more quickly than those of a Gaussian.\n",
    "\n",
    "The **quantile $x_{\\alpha}$** is the value of the random variable x at which the cumulative distribution is equal to $\\alpha$. That is, the quantile is the inverse of the cumulative distribution function, i.e., $x_{alpha} = F^{−1}(\\alpha)$. An important special case is the **median, $x_{med}$**, defined by $F(x_{med}) = 1/2$, i.e., half the probability lies above and half lies below $x_{med}$. (More rigorously, $x_{med}$ is a median if $P(x \\geq x_{med}) \\geq 1/2$ and $P(x \\leq x_{med}) \\geq 1/2$. If only one value exists, it is called ‘the median.’)\n",
    "\n",
    "Under a monotonic change of variable $x \\rightarrow y(x)$, the quantiles of a distribution (and hence also the median) obey $y_{\\alpha} = y(x_{\\alpha})$. In general the expectation value and **mode** (most probable value) of a distribution do not, however, transform in this way.\n",
    "\n",
    "Let us look at median and quantile$_68$ of the normal pdf: \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54598c68-45c5-479b-b6d2-4a78e884e778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.3.2 (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe03195-5ae8-424b-8b70-f0cb5971c29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get some desciptive statistics from a normal (continous) p.d.f. with mean 0 and SD 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44d35893-6ca0-4e38-9dfe-966d14402f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mean, variance, skewness, kurtosis = scipy.stats.norm.stats(0,4,moments='mvsk')\n",
    "print(mean, variance, skewness, kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75107e35-e239-4ee3-b8da-5d9f64c2b7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot the p.d.f and some moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb13010-7a7b-4d42-9bcf-2777271febc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-20,20,400)\n",
    "sigma=variance**0.5\n",
    "plt.plot(x,scipy.stats.norm.pdf(x,mean,sigma))\n",
    "plt.axvline(x=mean, linewidth=2, color = 'k',label=\"Mean\") # Plot the mean as a vertical line\n",
    "plt.axvline(x=mean-sigma, linewidth=2, color = 'r', label=\"Sigma (standard deviation)\")\n",
    "plt.axvline(x=mean+sigma, linewidth=2, color = 'r')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aea4ad74-9d87-4082-a27f-fecdecef0604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Do the same with the discrete and skew poisson p.m.f. See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3a2fe32-31b8-46db-bd10-3f1137fb2712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "m, v, s, k = scipy.stats.poisson.stats(1.7,moments='mvsk')\n",
    "#my_norm = norm(0,2)\n",
    "print('Mean = %1.2f Var = %1.2f Std = %1.2f Skewness = %1.2f kurtosis = %1.2f' % (m,v,v**0.5,s,k))\n",
    "#my_norm.moments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bc24f5-be4c-4ff3-a429-b3ef9028508f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot mode and median of a poisson p.d.f. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a230b800-0175-4892-9682-1dd01c6c2fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dist = scipy.stats.poisson(1.7)\n",
    "x = np.arange(-1, 10)\n",
    "sigma=variance**0.5\n",
    "plt.plot(x,dist.pmf(x),linestyle='steps-mid')\n",
    "dist.median()\n",
    "plt.axvline(x=mean, linewidth=2, color = 'k',label=\"Mean\")\n",
    "plt.axvline(x=mean+sigma, linewidth=2, color = 'r', label=\"Sigma (standard deviation)\")\n",
    "plt.axvline(x=dist.median(), linewidth=2, color = 'b',label=\"Median\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e00e199-7d20-4bd7-95a5-fb44b6db91f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Some words on the Poisson distribution**\n",
    "\n",
    "The Poisson distribution is popular for modelling the number of times an event occurs in an interval of time or space. For example:\n",
    "\n",
    "- The number of meteorites greater than 1 meter diameter that strike Earth in a year\n",
    "- The number of patients arriving in an emergency room between 10 and 11 pm\n",
    "\n",
    "The probability mass function is $$f(k;\\lambda) = \\frac{\\lambda^k exp(-\\lambda)}{k!}$$\n",
    "For large k the normal distribution is an excellent approximation of the poisson p.d.f. For k below 20 one should be careful using statements based on the normal distribution, e.g. the standard deviation is not symmetric anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ab3a62-a52b-48f9-91a7-f722b59549b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-1, 40)\n",
    "plt.plot(x,scipy.stats.poisson.pmf(x,2),linestyle='steps-mid')\n",
    "plt.plot(x,scipy.stats.poisson.pmf(x,15),linestyle='steps-mid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4fa3190-c8c8-4b89-b34c-186e1db85378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see that a possion p.d.f. with mean 15 looks very much like the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d2bf496-d526-4f23-bc03-e5c1a57de3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Some words on the binomial distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d36d570-fa27-4ab6-aba9-ce73a799b71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "$$f(k;n,p) = \\binom{n}{k} \\cdot p^kq^{n-k}$$\n",
    "\n",
    "The binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while the product np remains fixed or at least p tends to zero. Therefore, the Poisson distribution with parameter λ = np can be used as an approximation to B(n, p) of the binomial distribution if n is sufficiently large and p is sufficiently small. According to two rules of thumb, this approximation is good if n ≥ 20 and p ≤ 0.05, or if n ≥ 100 and np ≤ 10.\n",
    "\n",
    "For n>20 and p not too close to 1 or 0, the normal distribution is also here a good approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb527887-55a4-48c6-b5d3-35fd01457588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-1, 40)\n",
    "plt.plot(x,scipy.stats.binom.pmf(x,40,0.05),linestyle='steps-mid')\n",
    "plt.plot(x,scipy.stats.binom.pmf(x,40,0.2),linestyle='steps-mid')\n",
    "plt.plot(x,scipy.stats.binom.pmf(x,40,0.5),linestyle='steps-mid')\n",
    "plt.plot(x,scipy.stats.binom.pmf(x,40,0.8),linestyle='steps-mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df3dd26-6efc-4a00-bd90-5dddada2ba6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get yourself a cup of something and think about these questions.\n",
    "\n",
    "- Why are measured observables random variables.\n",
    "- Which probability distribution of a random variable is the most important?\n",
    "- Thumb of rule, when is the normal distribution a good approximation?\n",
    "- Can you mention 5 descriptive statistical measures (mostly based on the moments of the p.d.f)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c3e29d-4c32-4f1b-9889-c28f64837730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2929597a-0d44-45a6-9a3c-2dc9cc2539ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let us look at the descriptive statistics of the Iris Setosa data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac85f433-dab6-4295-a778-8327cb1a1275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('iris.csv',names=['slength','swidth','plength','pwidth','name'])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef96234-4706-41dc-bfec-2316fb53b1e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_setosa = dataframe[dataframe['name']=='Iris-setosa']\n",
    "df_setosa.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3a7841-6f40-4d80-8d2d-0eca2c98d6fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_setosa.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1c5f77-b384-48ef-bdff-13e547982340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_setosa.describe() # Or get a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96bf28e8-4075-49e3-825f-ea227542eb70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All the descriptive statistics methods for python dataframes are listed here:\n",
    "https://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d54e7f6-f7f3-4bd2-8eef-2a11828d882e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we looked at the numbers. Let's us plot the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e471fba-caca-41b9-a820-942c3d18eaa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_setosa['slength'].plot(kind=\"hist\",fill=False,histtype='step',title='Iris Setosa', label=\"length\")\n",
    "ax = df_setosa['swidth'].plot(kind=\"hist\",fill=False,histtype='step', label=\"width\")\n",
    "ax.set_xlabel('Setal length/width [cm]')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5a6b903-4b49-442d-8f03-48f28c2873a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_setosa['plength'].plot(kind=\"hist\",fill=False,histtype='step',title='Iris Setosa', label=\"length\")\n",
    "ax = df_setosa['pwidth'].plot(kind=\"hist\",fill=False,histtype='step', label=\"width\")\n",
    "ax.set_xlabel('Petal length/width [cm]')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4c1875-1ddd-40a9-a206-7f20233b6068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is hard to say by eye if the distributions are normally distributed. Later this week, we'll use statistical tests to find that out. The binning is also not equal. To define equal binnings see example from yesterday. The petal length is funny.\n",
    "\n",
    "We can complete our descriptive studies by looking at the scatter plots (correlations). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f17951e-c281-441b-b91c-ee3b7539c4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 2.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d61a89-7646-4927-b702-430f08120d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is important to obtain some routine with the computation of probabilities and quantiles. \n",
    "\n",
    "Let X be binomially distributed with n = 60 and p = 0.4. Compute the following.\n",
    "- P(X = 24) (PMF), P(X ≤ 24) (CDF)\n",
    "- Compute the mean and standard deviation of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f09486-e102-42a0-9998-1ae757f88a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb4d6e7c-0e44-47fe-982d-efe57471a708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4 How to generate random variates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c906741-7ea8-41d4-bfa6-9b3dbf5211c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One can simulate data sets by generating them from probability density functions. The computer does this with a so called Monte Carlo (MC) algorithm. It draws x values (pseudo) randomly from the given distribution. The the actual draws of the random variable are called random variates. Simulations can be very useful when planning an experiment and developing the analysis method. Instead of real data one can use the simulated data. \n",
    "\n",
    "Let us simulate the Iris setosa sepal width data (width the mean and standard deviation we got from the real data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff6c342-ae4e-4d95-b34b-6f3ef0a7fae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n = scipy.stats.norm.rvs(3.418,0.318,50) # 100 random values from a normal distribution with mean 3.418 and SD 0.318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "292660cc-cae5-4ca8-892b-3513b893772c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(n[0:10]) # Print first 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad9ba75-8d97-4b8a-bbfa-1d60c4ca5e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Put the simulated data into a dataframe\n",
    "import pandas as pd\n",
    "df_setosa_sim = pd.DataFrame(n)\n",
    "df_setosa_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58e79bbf-ced9-426c-af88-dd219452196a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.5 Other probability density functions (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2ef9ca8-fee8-4fcc-8c5a-234abeee0eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is just for your reference, no need to understand all these distributions. A quite good free and comprehensive book is here: http://staff.fysik.su.se/~walck/suf9601.pdf. \n",
    "\n",
    "**Table 1.4.1** Some common probability density functions, with corresponding characteristic functions and means and variances. In the Table, $\\Gamma(k)$ is the gamma function, equal to $(k − 1)!$ when $k$ is an integer; $_1F_1$ is the confluent hypergeometric function of the 1st kind [11].\n",
    "\n",
    "![download.png](attachment:download.png)\n",
    "\n",
    "Distributions are either discrete or continuous. To study the distributions we use the statistical functions of scipy.stats (https://docs.scipy.org/doc/scipy-0.14.0/reference/stats.html). There you also find more distributions than listed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8aa180-cd44-4c01-9eda-7a527b695dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.6 Uncertainties (important)\n",
    "\n",
    "All data have uncertainties. These should always be communicated when showing scientific numbers or plots. We distinguish between two types.\n",
    "\n",
    "- Statistical uncertainties \n",
    "    - Fluctuations, can be made smaller by taking more data, i.e. get more statistics\n",
    "- Systematic uncertainties \n",
    "    - Shift of data in one direction due to some \"mistake\" in the measurement, e.g. wrongly calibrated instrument showing all measured values systematically higher as they really are. Or for instance uncertainty due to the choice of methods and tools\n",
    "    \n",
    "The statistics tools can mostly handle the statistical uncertainties. There is no mathematical recipe for dealing with systematical uncertainties. You have to think through your experiment and try to estimate the influence of everything that can go wrong.\n",
    "\n",
    "When uncertainties are stated on numbers or in graphs as error bars or error bands they generally show one standard deviation. If the data are well described by a normal p.d.f, the interpretation of one standard deviation is clear: if the measurement is repeated many times, 32% (or about 1/3 of the measurements) **should** be outside the error bars. \n",
    "\n",
    "![Screen%20Shot%202018-05-29%20at%2021.34.12.png](attachment:Screen%20Shot%202018-05-29%20at%2021.34.12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f33a93c-9b5a-42ab-8f07-c6949b99ce70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If the p.d.f is not normal, the interpretation of one standard deviation is not clear. For example, for a poisson p.d.f. it is not symmetric. So again we see, for low counts where the normal asumption is not a good approximation, let's say below 20, the interpretation is not obvious anymore and care is needed.\n",
    "\n",
    "Very nice is the fact that the standard deviation of a poisson distribution is $\\sqrt{\\mu}$. So if you have a count that is larger than around 20 (then the normal interpretation is a good approximation), you get the standard deviation by taking the square root of the count. \n",
    "\n",
    "The following code may be difficult to understand, ask on the chat or tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0abf7f-044b-4e4f-ab23-e9591d0fa2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Draw a histogram which is not normalised\n",
    "entries1, edges, patches = plt.hist(n, bins=10, histtype='step')\n",
    "# Close plt so that this histogram is not shown\n",
    "plt.close() \n",
    "# Draw a histogram which IS normed\n",
    "entries2, edges, patches = plt.hist(n, bins=10, histtype='step',density=True)\n",
    "# Calculate the poisson standard deviation and scale down to second histogram\n",
    "errors = np.sqrt(entries1) * entries2/entries1\n",
    "# calculate bin centers\n",
    "bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "# draw errobars, use the sqrt error.\n",
    "plt.errorbar(bin_centers, entries2, yerr=errors, fmt='r.')\n",
    "# Draw a normal distribution\n",
    "x = np.linspace(2.4,4.2,100)\n",
    "sigma=variance**0.5\n",
    "plt.plot(x,scipy.stats.norm.pdf(x,df_setosa_sim.mean(),df_setosa_sim.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a8cebea-fcd8-4e34-a417-bd16f8871d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see that 3 out of 10 data points are more than one standard deviation off the \"theory\" curve. This is how it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ce9e0c-a793-4d24-b9dd-0145bfcfcdbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.7 Submit 1 question (10 min)  - mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b47138e7-06f5-481b-988f-90bee393895c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Submit one question to this notebook for tomorrow's discussion session: Link on Ilias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec4c347d-43e9-421a-86c7-87a69d54494c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0514912-a481-4781-9ec5-f22675892e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. H. Cramer, Mathematical Methods of Statistics, (Princeton Univ. Press, New Jersey, 1958).\n",
    "2. A. Stuart and J.K. Ord, Kendall’s Advanced Theory of Statistics, Vol. 1 Distribution Theory 6th Ed., (Halsted Press, New York, 1994), and earlier editions by Kendall and Stuart.\n",
    "3. R.J. Barlow, Statistics: A Guide to the Use of Statistical Methods in the Physical Sciences, (John Wiley, New York, 1989).\n",
    "4. S. Brandt, Data Analysis, 3rd Ed., (Springer, New York, 1999).\n",
    "5. G. Cowan, Statistical Data Analysis, (Oxford University Press, Oxford, 1998).\n",
    "6. A.N. Kolmogorov, Grundbegriffe der Wahrscheinlichkeitsrechnung, (Springer, Berlin,\n",
    "1933); Foundations of the Theory of Probability, 2nd Ed., (Chelsea, New York 1956).\n",
    "7. Ch. Walck, Hand-book on Statistical Distributions for Experimentalists, University of Stockholm Internal Report SUF-PFY/96-01, available from http://staff.fysik.su.se/~walck/suf9601.pdf.\n",
    "8. M. Abramowitz and I. Stegun, eds., Handbook of Mathematical Functions, (Dover, New York, 1972)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1cc4dc4-5ca4-4c65-a18b-9032ec88e270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Annex 1 Probability\n",
    "\n",
    "An abstract definition of probability can be given by considering a set $S$, called the sample space, and possible subsets $A,B,...$ the interpretation of which is left open. The probability $P$ is a real-valued function defined by the following axioms due to Kolmogorov (1933) [9]:\n",
    "\n",
    "- For every subset $A$ in $S$, $P(A) ≥ 0$;\n",
    "- For disjoint subsets (i.e., $A ∩ B = ∅$), $P(A ∪ B) = P(A) + P(B)$; \n",
    "- $P(S)=1$.\n",
    "\n",
    "From this further properties can be derives, e.g.\n",
    "\n",
    "- $P(\\bar{A}) = 1 - P(A)$\n",
    "- $P(A \\cup \\bar{A}) = 1$\n",
    "- $P(\\emptyset) = 0$\n",
    "- if A in B, then $P(A)\\leq P(B)$\n",
    "- $P(A \\cup \\bar{A}) = P(A) + P(B) - P(A\\cap B)$\n",
    "\n",
    "#### Conditional probability\n",
    "In addition, one defines the conditional probability $P(A|B)$ (read as $P$ of $A$ given $B$) as $$P(A|B) = \\frac{P(A ∩ B)}{P(B)}$$\n",
    "\n",
    "As an example, when throwing the dice, consider obtaining more than 3 eyes given only trows with even number of eyes outcomes. We calculate the (conditional) probability:\n",
    "\n",
    "$$P(n>3|n\\; even) = \\frac{P(n>3 \\cap n\\; even)}{P(even)} = \\frac{2/6}{3/6} = \\frac{2}{3}$$\n",
    "\n",
    "#### Independence\n",
    "\n",
    "If A and B are independent, then \n",
    "\n",
    "$$P(A|B) = \\frac{P(A ∩ B)}{P(B)} = \\frac{P(A)P(B)}{P(B)} = P(A)$$\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "CAS-D1-Probability",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
